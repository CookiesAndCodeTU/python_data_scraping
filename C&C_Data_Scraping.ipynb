{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "from functools import partial\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Link to sample data on Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/file/d/1rAqpbo3z-pjUNZizY0qgvMSIezLX_xlC/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Python Requests Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the Python 'requests' library to interact with the internet\n",
    "###### Requests Docs: https://requests.kennethreitz.org/en/master/#\n",
    "###### Requests Tutorial: https://realpython.com/python-requests/\n",
    "###### Requests Youtube tutorial: https://www.youtube.com/watch?v=tb8gHvYlCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a 'GET' request to the yahoo, should return '200'\n",
    "# we can use 'response' to access all elements found in the page source\n",
    "response = requests.get('https://finance.yahoo.com/quote/TSLA?p=TSLA', 'lxml')\n",
    "# this should redurrn '<Response [200]>' indicating a succsesfulll request and response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### '.text' method gives us the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are assigning text value of the reponse to variable 'response_text' \n",
    "response_text = response.text\n",
    "\n",
    "# notices that requests returns the page as a 'str'\n",
    "print(f'response_text type: {type(response_text)}')\n",
    "\n",
    "# prints out a spacer, makes output more readable\n",
    "print('–'*50)\n",
    "# print out \n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'beautifulsoup' module allows us to search the webpage by tag/selector\n",
    "Beautiful Soup module allows us to search webpages based on their html tags, classes, or ids\n",
    "###### BS Docs: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "###### BS Tutorial: https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3\n",
    "###### BS Youtube Tutorial: https://www.youtube.com/watch?v=ng2o98k983k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creates 'bs4.BeautifulSoup' Object\n",
    "source = bs.BeautifulSoup(response_text)\n",
    "\n",
    "# notices that 'source' type is a BeautifulSoup object, we can search this for specific elements\n",
    "print('SOURCE TYPE:\\n', type(source))\n",
    "\n",
    "# spacer( '\\n' means 'new line')\n",
    "print('\\n')\n",
    "\n",
    "# prints out the page source as a string 'str'\n",
    "print('SOURCE:\\n', source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finda all links in the webage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finds all elements with the 'a' tag\n",
    "links = source.find_all('a')\n",
    "\n",
    "# here we are 'itterating' over the 'list' of links, and printing out each 'link' in the list \n",
    "for link in links:\n",
    "    \n",
    "    # 'link' is any 'a' tag on the page source. This includes the url, its classes, ids and any other atributes \n",
    "    print('LINK:\\n', link)\n",
    "\n",
    "    # we are filtering link to just the 'href' atribute, this conatines the actual url\n",
    "    print('URL:\\n', link['href'])\n",
    "    \n",
    "    # print the text value of the link, this is what is displayed on the web page when you click on it\n",
    "    print('TEXT:\\n', link.text)\n",
    "    \n",
    "    # divider, makes output more readdable\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Get Company Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# takes any ticker symbol and returns financial data as a dictionary (‘dict’)\n",
    "# ':str' and '-->' are examples of function annotations. Can be accessed with '.__annotations__'\n",
    "# annotations give usefull information about the function\n",
    "def get_company_data(ticker: str) -> dict:\n",
    "    # this is called a 'doc string', a brief description of the function. Can be accessed with '.__doc__'\n",
    "    '''\n",
    "    Takes any ticker symbol and returns financial data as a dict\n",
    "    Parameters: A ticker symbol (str)\n",
    "    Returns: A dict of financial data from Yahoo Finance\n",
    "    '''\n",
    "    \n",
    "    # base url for yahoo financial stats\n",
    "    url = f'https://finance.yahoo.com/quote/{ticker}/key-statistics?p={ticker}'\n",
    "    \n",
    "    # header passed request\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    # makes request \n",
    "    response = requests.get(url, 'lxml', headers=headers)\n",
    "\n",
    "    # handles for bad url, response should be 200, anything else is an error\n",
    "    if response.status_code != 200:\n",
    "        return {'ticker': ticker, '!status': f'code {response.status_code}'}\n",
    "    \n",
    "    # main bs page object\n",
    "    source = bs.BeautifulSoup(response.text)\n",
    "    # find all 'section' tags with 'data-test' attribute of 'qsp-statistics'\n",
    "    data = source.find('section', {'data-test':'qsp-statistics'})\n",
    "    \n",
    "    # handles for invalid ticker symbol, \".find()\" returns \"None\" \n",
    "    if data == None:\n",
    "        return {'ticker': ticker, '!status': 'data == None'}\n",
    "    \n",
    "    # finds the company name: Selects all 'div' elements with an id of \"'id':'quote-header-info'\"\n",
    "    company_name = source.find('div', {'id':'quote-header-info'}).find('h1').text\n",
    "    \n",
    "    # creates a list of all 'tr'('table row') elements. Since we know we want all info in the tables,\n",
    "    # this is a good way to select the data. Note 'find_all' returns a 'list' of elements\n",
    "    rows = data.find_all('tr')\n",
    "\n",
    "    # Python dictionary object. '!' take are sorted first alphabetically, keeps in first column\n",
    "    info_dict = {'ticker': ticker, '!status': response.status_code, '!!company_name': company_name}\n",
    "    \n",
    "    # We are 'iterating' over the 'list' of 'tr' elements, selecting the category, and data value\n",
    "    for row in rows:\n",
    "        \n",
    "        # 'td' (table data) tag. Think of this as a cell in the table. These tables have two columns, \n",
    "        # the first column is the category name, the second is the data value,\n",
    "        # should returns a list with 2 elements\n",
    "        data = row.find_all('td')\n",
    "        \n",
    "        # we create 'key' variable in used in our dictionary. This contains name of the category\n",
    "        key = data[0].text.strip()\n",
    "        \n",
    "        # we create 'value' variable in used in our dictionary. This contains value of the category\n",
    "        value = data[1].text.strip()\n",
    "        \n",
    "        # add the key and value to the dictionary we created. \n",
    "        info_dict[key] = value\n",
    "        \n",
    "    # function returns the dictionary, if we set a variable equal to this function,\n",
    "    # that variable will be a dictionary ('dict' object)\n",
    "    return info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function annotations\n",
    "print(get_company_data.__annotations__)\n",
    "# function doc string\n",
    "print(get_company_data.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the 'Pandas' (imported as 'pd') module to organize the data into a useable table\n",
    "###### Pandas Docs: https://pandas.pydata.org/pandas-docs/stable/\n",
    "###### Pandas Tutorial: https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "###### Pandas Youtube Tutorial: https://www.youtube.com/watch?v=0UA49Ds1XXo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 'list' that containes three tickers ('str')\n",
    "ticker_list = ['tsla', 'goog', 'gs', 'fb']\n",
    "\n",
    "# an empty Python 'list'. This is used to store data temporaraly before being tunred into a Pandad 'DataFrame'\n",
    "company_data_list = []\n",
    "\n",
    "# we are 'itterating' over the 'ticker_list'\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "    # feeds each of the tickers in the list into the 'get_company_data' function we defined earlier\n",
    "    # and asign the return value of this function ('dict') to a variable 'company_data'\n",
    "    company_data = get_company_data(ticker)\n",
    "    \n",
    "    # adds the 'company_data' to the empty 'company_data_list' we created earlier\n",
    "    company_data_list.append(company_data)\n",
    "    \n",
    "    print(ticker.upper())\n",
    "    print(company_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable 'df' is created, this will be our 'DataFrame'\n",
    "# 'pd.DataFrame' is a pandas 'DataFrame' object. You can think of it  \n",
    "# much like an excel spreadsheet (rows and columns)\n",
    "# the dataframe is created by passing a 'list' of dictionaries ('dict') which pandas converts\n",
    "df = pd.DataFrame(company_data_list)\n",
    "\n",
    "# by default, pandas creates an numerical index (1,2,3...). We are setting the index to the 'ticker' column\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "# shape give us the rows and columns of the 'df'\n",
    "print(df.shape)\n",
    "\n",
    "# jupyter will display the 'df' if it is the last line in a cell, without useing the 'print' function\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Get List of S&P 500 Tickers\n",
    "###### Wikipedia list of S&P 500 Comapnies:  https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
    "###### Yahoo: https://finance.yahoo.com/quote/TSLA/key-statistics?p=TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of S&P 500 tickers from Wikipedia, returns dictionary of company ticker and name\n",
    "# Note this function takes no parameters, the url is included in the function,\n",
    "# will not work with other Wikipedia pages\n",
    "def get_tickers() -> list:\n",
    "    '''\n",
    "    Parameters: None\n",
    "    Returns: A list of dictionaries containing companies and their tickers\n",
    "    '''\n",
    "    # unlike the previous function the url is not going to change\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    \n",
    "    # optional headers, in this case we are changing the user agent\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    # request the Wikipedia page\n",
    "    response = requests.get(url, 'lxml', headers=headers)\n",
    "    \n",
    "    # bs object, searchable by 'dom' elements\n",
    "    source = bs.BeautifulSoup(response.text)  \n",
    "    \n",
    "    # find the first element with a 'table' tag, and a 'id' of 'constituents'\n",
    "    main_table = source.find('table', {'id': 'constituents'})\n",
    "    \n",
    "    # after finding the 'main_table', we select just the table body 'tbody'\n",
    "    table_body = main_table.find('tbody')\n",
    "    \n",
    "    # selects all rows 'tr' in the table body\n",
    "    rows = table_body.find_all('tr')\n",
    "    \n",
    "    # each ticker is added to this list\n",
    "    company_list = []\n",
    "    # itterate over the table rows, select each ticker symbol\n",
    "    for row in rows:\n",
    "        \n",
    "        # row contains list of 'td' (table data) elements\n",
    "        row_cells = row.find_all('td')\n",
    "        \n",
    "        # skips any row missing essential data (first and second column)\n",
    "        if len(row_cells) <= 1:\n",
    "            # 'continue' advances the for loop to the next item\n",
    "            continue\n",
    "                \n",
    "        # first column in table. '.strip' removes white space\n",
    "        ticker = row_cells[0].text.strip()\n",
    "        \n",
    "        # '.append' method adds a element ('ticker') to the end of a list ([company_list').\n",
    "        company_list.append(ticker)\n",
    "        \n",
    "    # return Python list containing all S&P 500 tickers  \n",
    "    return company_list\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runs the 'get_tickers()' function, creates list of tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creates a variable 'tickers_list' which is set equal to the output of 'get_tickers()' function\n",
    "tickers_list = get_tickers()\n",
    "\n",
    "# prints number of tickers in 'tickers_list'\n",
    "print(f\"{len(tickers_list)} tickers in list\")\n",
    "\n",
    "# prints the entire 'ticker_list'\n",
    "print(tickers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gets financial data for the first 10 tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gets data for first 10 on a single thread\n",
    "# 't0' is start time\n",
    "t0 = time.time()\n",
    "\n",
    "# data for each company will be added to this list\n",
    "company_data_list = []\n",
    "# itterate over the first 10 tickers in the tickers list.\n",
    "# '[:10]' is slice notation for first 10 elements in list\n",
    "for ticker in tickers_list[:10]:\n",
    "    \n",
    "    # variable 'company_data' is set to the return value of 'get_company_data' function, a dictionary\n",
    "    company_data = get_company_data(ticker)\n",
    "    \n",
    "    # add the company data dictionary to 'company_data_list'\n",
    "    company_data_list.append(company_data)\n",
    "    \n",
    "    print(ticker)\n",
    "    \n",
    "# 't1' is current time when this line is evaluated\n",
    "t1 = time.time()\n",
    "print(\"{:.4} seconds\".format(t1-t0))\n",
    "\n",
    "# makes 'DataFrame' from 'company_data_list'\n",
    "df = pd.DataFrame(company_data_list)\n",
    "\n",
    "# set the ticker column as the index\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "# df rows and columns\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prints the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df rows and columns\n",
    "print(df.shape)\n",
    "\n",
    "# prints df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Saving Data\n",
    "### convert the data to a CSV from the pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Working Directory. The current folder this file is in\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all files in directory before making the csv, 'enumerate' function numbers what is being looped over\n",
    "for index, file in enumerate(os.listdir()):\n",
    "    print(index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as CSV (Comma Seperated Values)\n",
    "df.to_csv(f'financial_data.csv')\n",
    "\n",
    "# save a file with the current datetime to prevent overriding an existing file\n",
    "df.to_csv(f'financial_data_{time.asctime()}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all files in directory before making the csv\n",
    "for index, file in enumerate(os.listdir()):\n",
    "    print(index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame from CSV, setting the index collumn to the first (0th) column in the CSV\n",
    "df2 = pd.read_csv('financial_data.csv', index_col=0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Multithread Requests (Thread pool executor)\n",
    "### Request multiple pages at once, saves time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re run 'get_tickers()'\n",
    "tickers_list = get_tickers()\n",
    "print(f\"{len(tickers_list)} tickers in list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  'thread_function ' makes multiple requests at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asynchronously call \"get_company_data\". Acts as a 'wrapper' for the other functions\n",
    "def thread_function(num, input_ticker_list=tickers_list, get_company_data_function=get_company_data) -> None:\n",
    "    \n",
    "    # ticker at index in list (num=5, 4th ticker in list, staring at 0)\n",
    "    ticker = input_ticker_list[num]\n",
    "    \n",
    "    # calls company data function\n",
    "    company_data = get_company_data_function(ticker)\n",
    "   \n",
    "    # adds 'compnay_data' to shared list\n",
    "    company_data_list.append(company_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runs 'thread_function' with 'tickers_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time\n",
    "t0 = time.time()\n",
    "\n",
    "# 'list' of dictionaries 'dict'\n",
    "company_data_list = []\n",
    "\n",
    "# max workers = max number of threads to be open at any one time\n",
    "# 'multithread' the requests. This means we are makeing multiple requests at once.\n",
    "# 'max_workers=5' is the max number of threads we can have open at ay one time.\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "\n",
    "    # 'mapping' the function to a range of integers. Eaxh 'int' corasponds to a index in the list\n",
    "    # 'partial' function allows us to pass one parameter to the functin before mapping it \n",
    "    executor.map(thread_function, range(10))\n",
    "\n",
    "# create the 'DataFrame'\n",
    "df = pd.DataFrame(company_data_list)\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "\n",
    "# end time\n",
    "t1 = time.time()\n",
    "# prints time it took for this cell to execute\n",
    "print(\"{:.4} seconds\".format(t1-t0))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filters the 'tickers_list' based on starting letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filters 'tickers_list' for tickers that start with given letter. * = 'args' = multiple arguments\n",
    "def starts_with(*letters, tickers_list=tickers_list): \n",
    "    \n",
    "    filtered_tickers = []\n",
    "    \n",
    "    for letter in letters:\n",
    "        filtered_tickers.extend([ticker for ticker in tickers_list if ticker.startswith(letter.upper())])  \n",
    "    return filtered_tickers\n",
    "\n",
    "letter_1 = 's'\n",
    "letter_2 = 't'\n",
    "letter_3 = 'r'\n",
    "\n",
    "letter_list = starts_with(letter_1, letter_2, letter_3)\n",
    "\n",
    "print(len(letter_list))\n",
    "print(letter_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'list' of dictionaries 'dict'\n",
    "company_data_list = []\n",
    "\n",
    "# max workers = max number of threads to be open at any one time\n",
    "# 'multithread' the requests. This means we are makeing multiple requests at once.\n",
    "# 'max_workers=5' is the max number of threads we can have open at ay one time.\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "\n",
    "    # 'mapping' the function to a range of integers. Eaxh 'int' corasponds to a index in the list\n",
    "    # 'partial' function allows us to pass one parameter to the functin before mapping it \n",
    "    executor.map(partial(thread_function, input_ticker_list=letter_list), range(len(letter_list)))\n",
    "\n",
    "# create the data frame\n",
    "df = pd.DataFrame(company_data_list)\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.index.str.startswith(letter_1.upper())]\n",
    "df2 = df[df.index.str.startswith(letter_2.upper())]\n",
    "df3 = df[df.index.str.startswith(letter_3.upper())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.head().index)\n",
    "print(df2.head().index)\n",
    "print(df3.head().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3['Profit Margin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(df[\"Profit Margin \"])\n",
    "df['PM_num'] = df['Profit Margin'].str.replace('%','') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['PM_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pm(df):\n",
    "    profit_margins = []\n",
    "    for ticker in df['Profit Margin']:\n",
    "        if type(ticker) == float or ('%' not in ticker): \n",
    "            continue\n",
    "        pm = float(ticker.replace('%',''))\n",
    "        profit_margins.append(pm)\n",
    "\n",
    "    return sum(profit_margins)/len(profit_margins)\n",
    "\n",
    "get_pm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [get_pm(df1), get_pm(df2), get_pm(df3)]\n",
    "x = [letter_1, letter_2, letter_3]\n",
    "\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x values\n",
    "labels = []\n",
    "# y values\n",
    "values = []\n",
    "\n",
    "for index, row in df1.iterrows():\n",
    "    pm = row['Profit Margin']\n",
    "        \n",
    "    if type(pm) == float or ('%' not in pm): \n",
    "        continue\n",
    "\n",
    "    pm = float(pm.replace('%',''))\n",
    "    profit_margins.append(pm)\n",
    "\n",
    "    labels.append(index)\n",
    "    values.append(pm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.bar(labels, values)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Profit margin (%)')\n",
    "plt.title('Profit margin by company')\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSJ Headline Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the saved csv files\n",
    "files = os.listdir(os.path.join(os.getcwd(), 'wsj_csvs'))\n",
    "\n",
    "file_list = []\n",
    "for file in files:\n",
    "    f = file.strip('.csv')\n",
    "    if f != \"DS_Store\":\n",
    "        file_list.append(f)\n",
    "        \n",
    "print(len(file_list))\n",
    "print(min(file_list))   \n",
    "print(max(file_list))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of pandas DataFrames from the WSJ CSVs\n",
    "df_list = []\n",
    "for file in files[:]:\n",
    "    df_list.append(pd.read_csv(f\"wsj_csvs/{file}\", index_col=0))\n",
    "print(len(df_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a master DataFrame with every wsj_csv\n",
    "df = pd.concat(df_list)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'].copy(), format='%Y%m%d')\n",
    "df.set_index('date', inplace=True, drop=True)\n",
    "df.sort_index()\n",
    "df['count'] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_one = 'Obama'\n",
    "keyword_two = 'Romney'\n",
    "frequency = '14D'\n",
    "window = 4\n",
    "# df = df.loc['2012-01-01':'2019-10-24']\n",
    "\n",
    "# keyword one\n",
    "df_1 = df[\n",
    "    df['summary'].str.contains(keyword_one) | \n",
    "    df['headline'].str.contains(keyword_one)\n",
    "].copy()\n",
    "df_1_re = pd.DataFrame(df_1['count'].resample(frequency).sum())\n",
    "df_1_re['SMA'] = df_1_re['count'].rolling(window=window).mean()\n",
    "\n",
    "# keyword two\n",
    "df_2 = df[\n",
    "    df['summary'].str.contains(keyword_two) |\n",
    "    df['headline'].str.contains(keyword_two)\n",
    "].copy()\n",
    "df_2_re = pd.DataFrame(df_2['count'].resample(frequency).sum())\n",
    "df_2_re['SMA'] = df_2_re['count'].rolling(window=window).mean()\n",
    "\n",
    "print(len(df_1))\n",
    "print(len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(df_1_re['count'], color='pink')\n",
    "plt.plot(df_1_re['SMA'], color='red')\n",
    "\n",
    "plt.plot(df_2_re['count'], color='lightblue')\n",
    "plt.plot(df_2_re['SMA'], color='blue')\n",
    "\n",
    "plt.legend((keyword_one, keyword_one+' SMA', keyword_two, keyword_two+' SMA'))\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel('Articles per period')\n",
    "plt.title('Word frequency in WSJ headlines')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Following sections will NOT work with Anaconda alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Using An In Memory Data Base (Redis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must start redis server in terminal first\n",
    "r_db = redis.Redis(port=6377, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a key value pair 'name' : 'Stefan'\n",
    "r_db.mset({\"name\": \"Stefan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key value from db\n",
    "r_db.mget('name')[0].decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearS db\n",
    "r_db.flushall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(tickers_list)} tickers in list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asynchronously call \"get_company_data\"\n",
    "def thread_map(num, input_tickers_list=tickers_list, get_company_data_function=get_company_data):\n",
    "    # ticker at point in list\n",
    "    ticker = input_tickers_list[num]\n",
    "    # calls company data function\n",
    "    company_data = get_company_data_function(ticker)\n",
    "    r_db.mset({ticker: str(company_data)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(thread_map, range(40))\n",
    "        \n",
    "t1 = time.time()\n",
    "print(\"{:.4} seconds\".format(t1-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create datadrame from data in redis db\n",
    "df = pd.DataFrame([json.loads(r_db.get(ticker).decode('UTF-8').replace(\"'\",'\"')) for ticker in r_db.keys()]).set_index('ticker')\n",
    "\n",
    "print(f\"{len(df.index)} Rows, {len(df.columns)} Columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common issues\n",
    "### Somtimes content on a webpage wont apear in the 'requests' response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.wsj.com/news/archive/20041001', 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = bs.BeautifulSoup(response.text)\n",
    "# fine all elements with an article tag\n",
    "articles = source.find_all('article')\n",
    "print(len(articles))\n",
    "for article in articles:\n",
    "    print(article.text)\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8: Selenium (render full web page before extracting data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define webdriver use firefox browser\n",
    "options = Options()\n",
    "# run without broswer window\n",
    "#options.add_argument('--headless')\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.wsj.com/news/archive/20080608')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get newspaper title and article summary from WSJ archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of dates for wsj archive url\n",
    "def create_date() -> list:\n",
    "    start_date = datetime.date(1996, 4, 6)\n",
    "    dates_list = []\n",
    "    while True:\n",
    "        start_date += datetime.timedelta(days=1)\n",
    "        dates_list.append(str(start_date).replace('-',''))\n",
    "        if datetime.date.today() == start_date:\n",
    "            break\n",
    "    return dates_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list list of possible dates fro url\n",
    "dates = create_date()\n",
    "print(f\"{len(dates)} total dates\")\n",
    "print(dates[1:3],dates[-3:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs_folder = 'wsj_csvs2'\n",
    "errors_folder = 'wsj_errors2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes directory for each days csvs\n",
    "if not os.path.exists(csvs_folder):\n",
    "    os.mkdir(csvs_folder)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes errors directory for each days csvs\n",
    "if not os.path.exists(errors_folder):\n",
    "    os.mkdir(errors_folder)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a formatted date, appends to WSJ archive url, returns df of days articles\n",
    "def get_days_news(date):\n",
    "\n",
    "    driver.get(f'https://www.wsj.com/news/archive/{date}')\n",
    "    raw_source = driver.page_source\n",
    "    source = bs.BeautifulSoup(raw_source)\n",
    "    articles = source.select(\"article[class*='WSJTheme--story']\")\n",
    "    \n",
    "    # if page does not load, date is added to error file\n",
    "    timeout = 0\n",
    "    # while article length is 0, the page waits until \n",
    "    while len(articles) == 0:\n",
    "        # brief pause, allows page to coninue loading\n",
    "        time.sleep(1)\n",
    "        # redifine the page source\n",
    "        raw_source = driver.page_source\n",
    "        # redifine Beautifule soup\n",
    "        source = bs.BeautifulSoup(raw_source)\n",
    "        articles = source.select(\"article[class*='WSJTheme--story']\")\n",
    "        timeout += 1\n",
    "        if timeout >= 20:\n",
    "            with open(os.path.join(os.getcwd(), errors_folder, f\"{date}.txt\"),'w') as f:\n",
    "                f.write(date)\n",
    "            return\n",
    "    \n",
    "    time.sleep(1)\n",
    "        \n",
    "    dict_list = []\n",
    "    for article in articles:\n",
    "        #print(article.text)\n",
    "        \n",
    "        # the tree sections of each article row\n",
    "        days_articles = {'section': article.select(\"div[class*='WSJTheme--flashline']\"),\n",
    "                         'headline': article.select(\"h3[class*='WSJTheme--headline']\"), \n",
    "                         'summary': article.select(\"p[class*='WSJTheme--summary']\")\n",
    "                        }\n",
    "        \n",
    "        # adds each of the three sections to dict, used for df\n",
    "        for item in days_articles:\n",
    "            if days_articles[item] == []:\n",
    "                days_articles[item] = 'None'\n",
    "            else:\n",
    "                days_articles[item] = days_articles[item][0].text\n",
    "         \n",
    "        # for date columns\n",
    "        days_articles['date'] = date\n",
    "        \n",
    "        # add to 'dict_list'\n",
    "        dict_list.append(days_articles)\n",
    "    \n",
    "    # creates pandas df from list of article dicts\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    \n",
    "    # add to csv\n",
    "    df.to_csv(os.path.join(os.getcwd(), csvs_folder, f\"{date}.csv\"))\n",
    "    \n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, date in enumerate(dates[1000:1005]):\n",
    "    get_days_news(date)\n",
    "    time.sleep(2)\n",
    "    print(index, date) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define webdriver use firefox browser\n",
    "options = Options()\n",
    "# run without broswer window\n",
    "#options.add_argument('--headless')\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.reddit.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images on page\n",
    "def get_img_links(num):\n",
    "    images = []\n",
    "    while len(images) < num:\n",
    "        raw_source = driver.page_source\n",
    "        source = bs.BeautifulSoup(raw_source)\n",
    "        images = source.select(\"img[class*='ImageBox-image'][class*='media-element']\")\n",
    "        driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "        time.sleep(.5)\n",
    "        print(len(images))\n",
    "    return [image['src'] for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save img to disk\n",
    "def save_img(url):\n",
    "    split_url = (url.split('?')[0]).split('.')\n",
    "    ext = split_url[-1]\n",
    "    name = split_url[-2].split('/')[1]\n",
    "    response = requests.get(url, stream=True)\n",
    "    path = 'images'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    with open(f'{path}/{name}.{ext}', 'wb') as image_file:\n",
    "        image_file.write(response.content)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creats list of all urls on the a redit page\n",
    "image_urls = get_img_links(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_url in image_urls:\n",
    "    save_img(image_url)\n",
    "    print(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://accounts.google.com/signin/v2/identifier?service=youtube&uilel=3&passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Faction_handle_signin%3Dtrue%26app%3Ddesktop%26hl%3Den%26next%3D%252F&hl=en&ec=65620&flowName=GlifWebSignIn&flowEntry=ServiceLogin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username/ password inputs\n",
    "username_input = driver.find_element_by_id('identifierId')\n",
    "username = getpass.getpass(prompt='Username', stream=None)\n",
    "username_input.clear()\n",
    "username_input.send_keys(username)\n",
    "submit = driver.find_element_by_id(\"identifierNext\")\n",
    "submit.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "password_input = driver.find_element_by_xpath(\"//input[@type='password']\")\n",
    "password = getpass.getpass(prompt='Password', stream=None)\n",
    "password_input.clear()\n",
    "password_input.send_keys(password)\n",
    "time.sleep(1)\n",
    "submit = driver.find_element_by_id(\"passwordNext\")\n",
    "submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.youtube.com/feed/trending?gl=US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = bs.BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = source.select(\"a[id='video-title'][class*='ytd-video-renderer'][aria-label*='']\")\n",
    "print(len(titles))\n",
    "for i in titles:\n",
    "    print (i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles[0:5]:\n",
    "    rel_link = title['href']\n",
    "    link = f'https://www.youtube.com{rel_link}'\n",
    "    driver.get(link)\n",
    "    time.sleep(.5)\n",
    "    buttons = []\n",
    "    timeout = 0\n",
    "    while len(buttons) == 0:\n",
    "        buttons = driver.find_elements_by_xpath(\"//button[starts-with(@aria-label,'like this')]\")\n",
    "        print(buttons)\n",
    "        timeout += 1\n",
    "        if timeout == 5:\n",
    "            break\n",
    "        time.sleep(.5)\n",
    "    \n",
    "    buttons[0].click()\n",
    "    time.sleep(1)\n",
    "    buttons[0].click()\n",
    "    print(link)\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes csv from wikipedia\n",
    "resp = requests.get('https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue', 'lxml')\n",
    "source = bs.BeautifulSoup(resp.text)\n",
    "table = source.select(\"table[class*='wikitable'][class*='wikitable']\")[0]\n",
    "rows = table.find_all('tr')\n",
    "header = table.find_all('th')    \n",
    "dict_list = []\n",
    "for row in rows:\n",
    "    data_dict = {}\n",
    "    tds = row.find_all(['td', 'th'])\n",
    "    for i, td in enumerate(tds):\n",
    "        data_dict[header[i].text.strip()] = td.text.strip()\n",
    "        \n",
    "    print(data_dict)\n",
    "    dict_list.append(data_dict)\n",
    "    \n",
    "df = pd.DataFrame(dict_list)\n",
    "df.set_index('Rank')\n",
    "df.to_csv('company_data.csv')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
